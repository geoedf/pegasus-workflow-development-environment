{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from IPython.core.display import SVG\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Containers\n",
    "\n",
    "The workflow illustrated here is a simple processing pipeline that uses a **Docker container** for two of its jobs. These jobs are colored in light blue. The first containerized job, `preprocess`, runs the executable `/usr/local/bin/preprocess.sh` inside of the container. Note that this executable is part of the container image. The second containerized job, `process_text_more`, transfers the executable `process_text_2nd_pass.py` via **HTTP** from `http://isi.edu/~tanaka/process_text_2nd_pass.py`, into the container where it will be executed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(\"diagram.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformation Catalog -----------------------------------------------\n",
    "tc = TransformationCatalog()\n",
    "\n",
    "# Create and add our containers to the TransformationCatalog.\n",
    "\n",
    "# A container that will be used to execute the following two transformations.\n",
    "tools_container = Container(\n",
    "                    \"tools-container\", \n",
    "                    Container.DOCKER, \n",
    "                    image=\"docker:///ryantanaka/preprocess:latest\"\n",
    "                )\n",
    "\n",
    "tc.add_containers(tools_container)\n",
    "\n",
    "# Create and add our transformations to the TransformationCatalog.\n",
    "\n",
    "# An executable that is installed inside of \"tools_container\".\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/local/bin/preprocess.sh\",\n",
    "                is_stageable=False,\n",
    "                container=tools_container\n",
    "            )\n",
    "\n",
    "# A shell script that can be staged to some site and executed.\n",
    "process_text = Transformation(\n",
    "                    \"process_text.sh\", \n",
    "                    site=\"local\", \n",
    "                    pfn=Path(\".\").resolve() / \"bin/process_text.sh\", \n",
    "                    is_stageable=True\n",
    "                )\n",
    "\n",
    "# A stageable python script that must be executed inside tools_container because\n",
    "# it contains packages that we have when we develop locally, but may not be \n",
    "# installed on a compute node. \n",
    "process_text_2nd_pass = Transformation(\n",
    "                            \"process_text_2nd_pass.py\",\n",
    "                            site=\"workflow-cloud\",\n",
    "                            pfn=\"http://www.isi.edu/~tanaka/process_text_2nd_pass.py\",\n",
    "                            is_stageable=True,\n",
    "                            container=tools_container\n",
    "                        )\n",
    "\n",
    "# An binary that is already installed on the condorpool site.\n",
    "tar = Transformation(\n",
    "        \"tar\",\n",
    "        site=\"condorpool\",\n",
    "        pfn=\"/usr/bin/tar\",\n",
    "        is_stageable=False\n",
    "    )\n",
    "\n",
    "tc.add_transformations(preprocess, process_text, process_text_2nd_pass, tar)\n",
    "tc.write()\n",
    "\n",
    "# --- Replica Catalog ----------------------------------------------------------\n",
    "initial_input_file = File(\"initial_input_file.txt\").add_metadata(size=54)\n",
    "\n",
    "rc = ReplicaCatalog()\\\n",
    "        .add_replica(\"local\", \"initial_input_file.txt\", Path(\".\").resolve() / \"initial_input_file.txt\")\\\n",
    "        .write()\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"docker-wf\")\n",
    "\n",
    "preprocessed_data = File(\"preprocessed_data.txt\")\n",
    "\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(initial_input_file, preprocessed_data)\\\n",
    "                    .add_inputs(initial_input_file)\\\n",
    "                    .add_outputs(preprocessed_data)\n",
    "\n",
    "processed_data = File(\"processed_data.txt\")\n",
    "\n",
    "job_process_text = Job(process_text)\\\n",
    "                    .add_args(preprocessed_data, processed_data)\\\n",
    "                    .add_inputs(preprocessed_data)\\\n",
    "                    .add_outputs(processed_data)\n",
    "\n",
    "twice_processed_data = File(\"twice_processed_data.txt\")\n",
    "extra_copy = File(\"backup.txt\")\n",
    "\n",
    "job_process_text_more = Job(process_text_2nd_pass)\\\n",
    "                            .add_args(processed_data, twice_processed_data, extra_copy)\\\n",
    "                            .add_inputs(processed_data)\\\n",
    "                            .add_outputs(twice_processed_data, extra_copy)\n",
    "\n",
    "result = File(\"scientific_results.tar.gz\")\n",
    "compress = Job(tar, _id=\"tar_job\")\\\n",
    "            .add_args(\"-cvzf\", result, twice_processed_data, extra_copy)\\\n",
    "            .add_inputs(*job_process_text_more.get_outputs())\\\n",
    "            .add_outputs(result)\n",
    "\n",
    "wf.add_jobs(\n",
    "    job_preprocess,\n",
    "    job_process_text,\n",
    "    job_process_text_more,\n",
    "    compress\n",
    ")\n",
    "\n",
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of `initial_input_file.txt` before it went through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat initial_input_file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the final output file, `scientific_results.tar.gz`, to view the contents of `twice_processed_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf ./wf-output/scientific_results.tar.gz > /dev/null 2>&1 && cat twice_processed_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
