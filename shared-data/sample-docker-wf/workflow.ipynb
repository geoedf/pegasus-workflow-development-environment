{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from IPython.core.display import SVG\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Containers\n",
    "\n",
    "The workflow illustrated here is a simple processing pipeline that uses a **Docker container** for two of its jobs. These jobs are colored in light blue. The first containerized job, `preprocess`, runs the executable `/usr/local/bin/preprocess.sh` inside of the container. Note that this executable is part of the container image. The second containerized job, `process_text_more`, transfers the executable `process_text_2nd_pass.py` via **HTTP** from `http://isi.edu/~tanaka/process_text_2nd_pass.py`, into the container where it will be executed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg viewBox=\"0.00 0.00 1902.00 103.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><g class=\"graph\" id=\"graph1\" transform=\"scale(1 1) rotate(0) translate(4 99)\"><title>G</title><polygon fill=\"white\" points=\"-4,5 -4,-99 1899,-99 1899,5 -4,5\" stroke=\"white\"/><!-- initial_input_file.txt --><g class=\"node\" id=\"node2\"><title>initial_input_file.txt</title><polygon fill=\"none\" points=\"128.387,-90 -0.386983,-90 -0.386983,-54 128.387,-54 128.387,-90\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-67.8\">initial_input_file.txt</text></g><!-- preprocess --><g class=\"node\" id=\"node8\"><title>preprocess</title><ellipse cx=\"275\" cy=\"-72\" fill=\"powderblue\" rx=\"108.677\" ry=\"23.52\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275\" y=\"-76.2\">preprocess</text><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275\" y=\"-59.4\">/usr/local/bin/preprocess.sh</text></g><!-- initial_input_file.txt&#45;&gt;preprocess --><g class=\"edge\" id=\"edge3\"><title>initial_input_file.txt-&gt;preprocess</title><path d=\"M128.274,-72C137.097,-72 146.387,-72 155.829,-72\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"156.119,-75.5001 166.118,-72 156.118,-68.5001 156.119,-75.5001\" stroke=\"black\"/></g><!-- preprocessed_data.txt --><g class=\"node\" id=\"node3\"><title>preprocessed_data.txt</title><polygon fill=\"none\" points=\"559.67,-90 422.33,-90 422.33,-54 559.67,-54 559.67,-90\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"491\" y=\"-67.8\">preprocessed_data.txt</text></g><!-- process_text --><g class=\"node\" id=\"node14\"><title>process_text</title><ellipse cx=\"658\" cy=\"-72\" fill=\"none\" rx=\"59.1083\" ry=\"18\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-67.8\">process_text</text></g><!-- preprocessed_data.txt&#45;&gt;process_text --><g class=\"edge\" id=\"edge7\"><title>preprocessed_data.txt-&gt;process_text</title><path d=\"M560.064,-72C569.452,-72 579.097,-72 588.493,-72\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"588.566,-75.5001 598.566,-72 588.566,-68.5001 588.566,-75.5001\" stroke=\"black\"/></g><!-- processed_data.txt --><g class=\"node\" id=\"node4\"><title>processed_data.txt</title><polygon fill=\"none\" points=\"897.795,-90 778.205,-90 778.205,-54 897.795,-54 897.795,-90\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-67.8\">processed_data.txt</text></g><!-- process_text_more --><g class=\"node\" id=\"node9\"><title>process_text_more</title><ellipse cx=\"1236\" cy=\"-41\" fill=\"powderblue\" rx=\"83.9489\" ry=\"18\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1236\" y=\"-36.8\">process_text_more</text></g><!-- processed_data.txt&#45;&gt;process_text_more --><g class=\"edge\" id=\"edge11\"><title>processed_data.txt-&gt;process_text_more</title><path d=\"M898.068,-67.8467C957.638,-63.5989 1052.23,-56.6919 1134,-50 1138.69,-49.6163 1143.5,-49.2147 1148.35,-48.8031\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1148.69,-52.287 1158.36,-47.9466 1148.09,-45.3125 1148.69,-52.287\" stroke=\"black\"/></g><!-- twice_processed_data.txt --><g class=\"node\" id=\"node5\"><title>twice_processed_data.txt</title><polygon fill=\"none\" points=\"1515.61,-94 1358.39,-94 1358.39,-58 1515.61,-58 1515.61,-94\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1437\" y=\"-71.8\">twice_processed_data.txt</text></g><!-- compress_results --><g class=\"node\" id=\"node20\"><title>compress_results</title><ellipse cx=\"1632\" cy=\"-52\" fill=\"none\" rx=\"77.5839\" ry=\"18\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1632\" y=\"-47.8\">compress_results</text></g><!-- twice_processed_data.txt&#45;&gt;compress_results --><g class=\"edge\" id=\"edge17\"><title>twice_processed_data.txt-&gt;compress_results</title><path d=\"M1515.67,-66.342C1527.94,-64.8156 1540.64,-63.2366 1552.92,-61.709\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1553.63,-65.1476 1563.13,-60.4403 1552.77,-58.2011 1553.63,-65.1476\" stroke=\"black\"/></g><!-- backup.txt --><g class=\"node\" id=\"node6\"><title>backup.txt</title><polygon fill=\"none\" points=\"1474.21,-40 1399.79,-40 1399.79,-4 1474.21,-4 1474.21,-40\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1437\" y=\"-17.8\">backup.txt</text></g><!-- backup.txt&#45;&gt;compress_results --><g class=\"edge\" id=\"edge19\"><title>backup.txt-&gt;compress_results</title><path d=\"M1474.3,-27.6428C1497.65,-31.2715 1528.82,-36.1167 1557.21,-40.5298\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1556.7,-43.9924 1567.12,-42.07 1557.77,-37.0755 1556.7,-43.9924\" stroke=\"black\"/></g><!-- scientific_results.tar.gz --><g class=\"node\" id=\"node7\"><title>scientific_results.tar.gz</title><polygon fill=\"none\" points=\"1893.84,-70 1748.16,-70 1748.16,-34 1893.84,-34 1893.84,-70\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1821\" y=\"-47.8\">scientific_results.tar.gz</text></g><!-- preprocess&#45;&gt;preprocessed_data.txt --><g class=\"edge\" id=\"edge5\"><title>preprocess-&gt;preprocessed_data.txt</title><path d=\"M383.627,-72C393.155,-72 402.677,-72 411.89,-72\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"412.144,-75.5001 422.144,-72 412.144,-68.5001 412.144,-75.5001\" stroke=\"black\"/></g><!-- process_text_more&#45;&gt;twice_processed_data.txt --><g class=\"edge\" id=\"edge13\"><title>process_text_more-&gt;twice_processed_data.txt</title><path d=\"M1301.33,-52.3152C1316.24,-54.9365 1332.35,-57.7703 1348.07,-60.5354\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1347.88,-64.0545 1358.33,-62.3397 1349.09,-57.1603 1347.88,-64.0545\" stroke=\"black\"/></g><!-- process_text_more&#45;&gt;backup.txt --><g class=\"edge\" id=\"edge15\"><title>process_text_more-&gt;backup.txt</title><path d=\"M1313.09,-33.7356C1338.87,-31.2736 1366.88,-28.5993 1389.64,-26.4262\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1390.08,-29.9003 1399.7,-25.4656 1389.41,-22.932 1390.08,-29.9003\" stroke=\"black\"/></g><!-- process_text_2nd_pass.py --><g class=\"node\" id=\"node10\"><title>process_text_2nd_pass.py</title><polygon fill=\"darkseagreen\" points=\"913.079,-36 756.921,-36 756.921,0 919.079,0 919.079,-30 913.079,-36\" stroke=\"black\"/><polyline fill=\"none\" points=\"913.079,-36 913.079,-30 \" stroke=\"black\"/><polyline fill=\"none\" points=\"919.079,-30 913.079,-30 \" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-13.8\">process_text_2nd_pass.py</text></g><!-- process_text_2nd_pass.py&#45;&gt;process_text_more --><g class=\"edge\" id=\"edge23\"><title>process_text_2nd_pass.py-&gt;process_text_more</title><path d=\"M919.281,-17.5738C978.686,-17.8908 1061.52,-19.6586 1134,-26 1142.19,-26.7163 1150.72,-27.6651 1159.21,-28.7387\" fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\"/><polygon fill=\"black\" points=\"1158.75,-32.2089 1169.13,-30.0499 1159.67,-25.2693 1158.75,-32.2089\" stroke=\"black\"/><text font-family=\"Times,serif\" font-size=\"10.00\" text-anchor=\"middle\" x=\"1036\" y=\"-41\">staged into the container from</text><text font-family=\"Times,serif\" font-size=\"10.00\" text-anchor=\"middle\" x=\"1036\" y=\"-29\"> http://isi.edu/~tanaka/process_text_2nd_pass.py</text></g><!-- process_text&#45;&gt;processed_data.txt --><g class=\"edge\" id=\"edge9\"><title>process_text-&gt;processed_data.txt</title><path d=\"M717.777,-72C733.794,-72 751.294,-72 767.83,-72\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"767.981,-75.5001 777.981,-72 767.981,-68.5001 767.981,-75.5001\" stroke=\"black\"/></g><!-- compress_results&#45;&gt;scientific_results.tar.gz --><g class=\"edge\" id=\"edge21\"><title>compress_results-&gt;scientific_results.tar.gz</title><path d=\"M1709.6,-52C1718.93,-52 1728.5,-52 1737.91,-52\" fill=\"none\" stroke=\"black\"/><polygon fill=\"black\" points=\"1738.04,-55.5001 1748.04,-52 1738.04,-48.5001 1738.04,-55.5001\" stroke=\"black\"/></g></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(\"diagram.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword $defs - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2020.08.07 04:09:21.585 UTC:\n",
      "2020.08.07 04:09:21.591 UTC:   -----------------------------------------------------------------------\n",
      "2020.08.07 04:09:21.597 UTC:   File for submitting this DAG to HTCondor           : docker-wf-0.dag.condor.sub\n",
      "2020.08.07 04:09:21.602 UTC:   Log of DAGMan debugging messages                 : docker-wf-0.dag.dagman.out\n",
      "2020.08.07 04:09:21.608 UTC:   Log of HTCondor library output                     : docker-wf-0.dag.lib.out\n",
      "2020.08.07 04:09:21.614 UTC:   Log of HTCondor library error messages             : docker-wf-0.dag.lib.err\n",
      "2020.08.07 04:09:21.620 UTC:   Log of the life of condor_dagman itself          : docker-wf-0.dag.dagman.log\n",
      "2020.08.07 04:09:21.625 UTC:\n",
      "2020.08.07 04:09:21.631 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2020.08.07 04:09:21.643 UTC:   -----------------------------------------------------------------------\n",
      "2020.08.07 04:09:22.658 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.08.07 04:09:23.568 UTC:   Created Pegasus database in: sqlite:////home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002/docker-wf-0.replicas.db\n",
      "2020.08.07 04:09:23.574 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.08.07 04:09:23.623 UTC:   Output replica catalog set to jdbc:sqlite:/home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002/docker-wf-0.replicas.db\n",
      "2020.08.07 04:09:23.796 UTC:   Submitting to condor docker-wf-0.dag.condor.sub\n",
      "2020.08.07 04:09:23.817 UTC:\n",
      "2020.08.07 04:09:23.824 UTC:   Your workflow has been started and is running in the base directory:\n",
      "2020.08.07 04:09:23.829 UTC:\n",
      "2020.08.07 04:09:23.835 UTC:   /home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002\n",
      "2020.08.07 04:09:23.840 UTC:\n",
      "2020.08.07 04:09:23.847 UTC:   *** To monitor the workflow you can run ***\n",
      "2020.08.07 04:09:23.852 UTC:\n",
      "2020.08.07 04:09:23.858 UTC:   pegasus-status -l /home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002\n",
      "2020.08.07 04:09:23.864 UTC:\n",
      "2020.08.07 04:09:23.869 UTC:   *** To remove your workflow run ***\n",
      "2020.08.07 04:09:23.875 UTC:\n",
      "2020.08.07 04:09:23.880 UTC:   pegasus-remove /home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002\n",
      "2020.08.07 04:09:24.779 UTC:   Time taken to execute is 3.957 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m#########\u001b[0m-----------------------------------------]  18.2% ..Failure (\u001b[1;32mCompleted: 4\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 1\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# pegasus-analyzer #\n",
      "####################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "************************************Summary*************************************\n",
      "\n",
      "Submit Directory   : /home/scitech/shared-data/sample-docker-wf/scitech/pegasus/docker-wf/run0002\n",
      "Total jobs         :     22 (100.00%)\n",
      "# jobs succeeded   :      4 (18.18%)\n",
      "# jobs failed      :      0 (0.00%)\n",
      "# jobs held        :      0 (0.00%)\n",
      "# jobs unsubmitted :     18 (81.82%)\n",
      "\n",
      "\n",
      "\n",
      "######################\n",
      "# pegasus-statistics #\n",
      "######################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "#\n",
      "# Pegasus Workflow Management System - http://pegasus.isi.edu\n",
      "#\n",
      "# Workflow summary:\n",
      "#   Summary of the workflow execution. It shows total\n",
      "#   tasks/jobs/sub workflows run, how many succeeded/failed etc.\n",
      "#   In case of hierarchical workflow the calculation shows the\n",
      "#   statistics across all the sub workflows.It shows the following\n",
      "#   statistics about tasks, jobs and sub workflows.\n",
      "#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.\n",
      "#     * Failed - total count of failed tasks/jobs/sub workflows.\n",
      "#     * Incomplete - total count of tasks/jobs/sub workflows that are\n",
      "#       not in succeeded or failed state. This includes all the jobs\n",
      "#       that are not submitted, submitted but not completed etc. This\n",
      "#       is calculated as  difference between 'total' count and sum of\n",
      "#       'succeeded' and 'failed' count.\n",
      "#     * Total - total count of tasks/jobs/sub workflows.\n",
      "#     * Retries - total retry count of tasks/jobs/sub workflows.\n",
      "#     * Total+Retries - total count of tasks/jobs/sub workflows executed\n",
      "#       during workflow run. This is the cumulative of retries,\n",
      "#       succeeded and failed count.\n",
      "# Workflow wall time:\n",
      "#   The wall time from the start of the workflow execution to the end as\n",
      "#   reported by the DAGMAN.In case of rescue dag the value is the\n",
      "#   cumulative of all retries.\n",
      "# Cumulative job wall time:\n",
      "#   The sum of the wall time of all jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job wall time as seen from submit side:\n",
      "#   The sum of the wall time of all jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "# Cumulative job badput wall time:\n",
      "#   The sum of the wall time of all failed jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job badput wall time as seen from submit side:\n",
      "#   The sum of the wall time of all failed jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job badput wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "------------------------------------------------------------------------------\n",
      "Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries\n",
      "Tasks          0         0       4           4         0         0\n",
      "Jobs           4         0       18          22        0         4\n",
      "Sub-Workflows  0         0       0           0         0         0\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Workflow wall time                                       : 3 mins, 22 secs\n",
      "Cumulative job wall time                                 : 32.24 secs\n",
      "Cumulative job wall time as seen from submit side        : 33.0 secs\n",
      "Cumulative job badput wall time                          : 0.0 secs\n",
      "Cumulative job badput wall time as seen from submit side : 0.0 secs\n",
      "\n",
      "# Integrity Metrics\n",
      "# Number of files for which checksums were compared/computed along with total time spent doing it.\n",
      "4 files checksums generated with total duration of 1.08 secs\n",
      "\n",
      "# Integrity Errors\n",
      "# Total:\n",
      "#       Total number of integrity errors encountered across all job executions(including retries) of a workflow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Transformation Catalog -----------------------------------------------\n",
    "tc = TransformationCatalog()\n",
    "\n",
    "# Create and add our containers to the TransformationCatalog.\n",
    "\n",
    "# A container that will be used to execute the following two transformations.\n",
    "tools_container = Container(\n",
    "                    \"tools-container\", \n",
    "                    Container.DOCKER, \n",
    "                    image=\"docker:///ryantanaka/preprocess:latest\"\n",
    "                )\n",
    "\n",
    "tc.add_containers(tools_container)\n",
    "\n",
    "# Create and add our transformations to the TransformationCatalog.\n",
    "\n",
    "# An executable that is installed inside of \"tools_container\".\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/local/bin/preprocess.sh\",\n",
    "                is_stageable=False,\n",
    "                container=tools_container\n",
    "            )\n",
    "\n",
    "# A shell script that can be staged to some site and executed.\n",
    "process_text = Transformation(\n",
    "                    \"process_text.sh\", \n",
    "                    site=\"local\", \n",
    "                    pfn=Path(\".\").resolve() / \"bin/process_text.sh\", \n",
    "                    is_stageable=True\n",
    "                )\n",
    "\n",
    "# A stageable python script that must be executed inside tools_container because\n",
    "# it contains packages that we have when we develop locally, but may not be \n",
    "# installed on a compute node. \n",
    "process_text_2nd_pass = Transformation(\n",
    "                            \"process_text_2nd_pass.py\",\n",
    "                            site=\"workflow-cloud\",\n",
    "                            pfn=\"http://www.isi.edu/~tanaka/process_text_2nd_pass.py\",\n",
    "                            is_stageable=True,\n",
    "                            container=tools_container\n",
    "                        )\n",
    "\n",
    "# An binary that is already installed on the condorpool site.\n",
    "tar = Transformation(\n",
    "        \"tar\",\n",
    "        site=\"condorpool\",\n",
    "        pfn=\"/usr/bin/tar\",\n",
    "        is_stageable=False\n",
    "    )\n",
    "\n",
    "tc.add_transformations(preprocess, process_text, process_text_2nd_pass, tar)\n",
    "tc.write()\n",
    "\n",
    "# --- Replica Catalog ----------------------------------------------------------\n",
    "initial_input_file = File(\"initial_input_file.txt\").add_metadata(size=54)\n",
    "\n",
    "rc = ReplicaCatalog()\\\n",
    "        .add_replica(\"local\", \"initial_input_file.txt\", Path(\".\").resolve() / \"initial_input_file.txt\")\\\n",
    "        .write()\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"dagman.retry\"] = \"1\"\n",
    "props[\"pegasus.transfer.arguments\"] = \"-m 1\"\n",
    "props.write()\n",
    "\n",
    "condorpool = Site(\"condorpool\")\\\n",
    "                .add_pegasus_profile(style=\"condor\")\\\n",
    "                .add_pegasus_profile(auxillary_local=\"true\")\\\n",
    "                .add_condor_profile(universe=\"vanilla\")\n",
    "sc = SiteCatalog()\n",
    "sc.add_sites(condorpool)\n",
    "sc.write()\n",
    "\n",
    "wf = Workflow(\"docker-wf\")\n",
    "\n",
    "preprocessed_data = File(\"preprocessed_data.txt\")\n",
    "\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(initial_input_file, preprocessed_data)\\\n",
    "                    .add_inputs(initial_input_file)\\\n",
    "                    .add_outputs(preprocessed_data)\n",
    "\n",
    "processed_data = File(\"processed_data.txt\")\n",
    "\n",
    "job_process_text = Job(process_text)\\\n",
    "                    .add_args(preprocessed_data, processed_data)\\\n",
    "                    .add_inputs(preprocessed_data)\\\n",
    "                    .add_outputs(processed_data)\n",
    "\n",
    "twice_processed_data = File(\"twice_processed_data.txt\")\n",
    "extra_copy = File(\"backup.txt\")\n",
    "\n",
    "job_process_text_more = Job(process_text_2nd_pass)\\\n",
    "                            .add_args(processed_data, twice_processed_data, extra_copy)\\\n",
    "                            .add_inputs(processed_data)\\\n",
    "                            .add_outputs(twice_processed_data, extra_copy)\n",
    "\n",
    "result = File(\"scientific_results.tar.gz\")\n",
    "compress = Job(tar, _id=\"tar_job\")\\\n",
    "            .add_args(\"-cvzf\", result, twice_processed_data, extra_copy)\\\n",
    "            .add_inputs(*job_process_text_more.get_outputs())\\\n",
    "            .add_outputs(result)\n",
    "\n",
    "wf.add_jobs(\n",
    "    job_preprocess,\n",
    "    job_process_text,\n",
    "    job_process_text_more,\n",
    "    compress\n",
    ")\n",
    "\n",
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
